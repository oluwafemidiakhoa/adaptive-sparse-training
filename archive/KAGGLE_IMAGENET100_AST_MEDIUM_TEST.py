"""
üöÄ MEDIUM TEST - COSINE ANNEALING + RANDAUGMENT (5 EPOCHS)
====================================================================
PURPOSE: Test LR scheduling + stronger augmentation in 30 minutes

Changes from QUICK TEST:
‚úÖ FIXED: Gradient-magnitude based significance (NOT image std)
‚úÖ FIXED: Loss magnitude scoring (NOT batch-normalized)
‚úÖ Added: Prediction entropy for uncertainty estimation
üÜï NEW: Cosine annealing LR scheduler with warmup
üÜï NEW: RandAugment for stronger data augmentation
‚è±Ô∏è  Runtime: ~30 minutes (5 epochs)

Expected Results:
- Validation Accuracy: 35-42% (up from 32-38% in Quick Test)
- Energy Savings: 89-91% (same)
- Activation Rate: 9-12% (same)

If accuracy improves by 9-16%, proceed to Production!
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from torchvision.models import resnet50, ResNet50_Weights
from pathlib import Path
from PIL import Image
import time
import numpy as np
import math

# Visualization
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.gridspec import GridSpec

# ============================================================================
# CONFIGURATION
# ============================================================================

class Config:
    """Experiment configuration - Enhanced with LR scheduling"""
    # Dataset - UPDATE THIS PATH
    data_dir = "/kaggle/input/imagenet100/ImageNet100"
    num_classes = 100
    image_size = 224

    # Training
    batch_size = 64
    num_epochs = 5  # ‚ö†Ô∏è MEDIUM TEST: 5 epochs for quick validation
    learning_rate = 0.001
    weight_decay = 1e-4

    # üÜï LR Scheduler - Cosine Annealing with Warmup
    lr_warmup_epochs = 1  # Warmup for first epoch
    lr_min = 1e-6  # Minimum learning rate

    # üÜï RandAugment Settings
    randaug_n = 2  # Number of augmentation transformations
    randaug_m = 9  # Magnitude of augmentations

    # AST Configuration
    target_activation_rate = 0.10
    initial_threshold = 2.5

    # PI Controller - Same as ULTIMATE
    adapt_kp = 0.02
    adapt_ki = 0.001
    ema_alpha = 0.2

    # Energy Model
    energy_per_activation = 1.0
    energy_per_skip = 0.01

    # System
    device = "cuda" if torch.cuda.is_available() else "cpu"
    num_workers = 2
    pin_memory = True

# ============================================================================
# DATASET WITH RANDAUGMENT
# ============================================================================

class RandAugment:
    """Simple RandAugment implementation"""
    def __init__(self, n, m):
        self.n = n
        self.m = m
        self.augment_list = [
            transforms.AutoAugment(),
        ]

    def __call__(self, img):
        # For simplicity, use AutoAugment which has similar effect
        # In production, use torchvision.transforms.RandAugment
        ops = transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET)
        return ops(img)

class ImageNet100Dataset(Dataset):
    """ImageNet-100 dataset loader"""
    def __init__(self, root_dir, split='train', transform=None):
        self.root_dir = Path(root_dir) / split
        self.transform = transform
        self.samples = []
        self.class_to_idx = {}

        for idx, class_dir in enumerate(sorted(self.root_dir.iterdir())):
            if class_dir.is_dir():
                self.class_to_idx[class_dir.name] = idx
                for img_path in class_dir.glob("*.JPEG"):
                    self.samples.append((str(img_path), idx))

        print(f"Loaded {len(self.samples)} images from {split} split")
        print(f"Found {len(self.class_to_idx)} classes")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        return image, label

def get_dataloaders(config):
    """Create ImageNet-100 dataloaders with RandAugment"""
    normalize = transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )

    # üÜï STRONGER AUGMENTATION with RandAugment
    train_transform = transforms.Compose([
        transforms.RandomResizedCrop(config.image_size),
        transforms.RandomHorizontalFlip(),
        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),  # RandAugment substitute
        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),
        transforms.ToTensor(),
        normalize,
    ])

    val_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(config.image_size),
        transforms.ToTensor(),
        normalize,
    ])

    train_dataset = ImageNet100Dataset(config.data_dir, 'train', train_transform)
    val_dataset = ImageNet100Dataset(config.data_dir, 'val', val_transform)

    train_loader = DataLoader(train_dataset, batch_size=config.batch_size,
                              shuffle=True, num_workers=config.num_workers,
                              pin_memory=config.pin_memory)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size,
                           shuffle=False, num_workers=config.num_workers,
                           pin_memory=config.pin_memory)

    return train_loader, val_loader

# ============================================================================
# LR SCHEDULER WITH WARMUP
# ============================================================================

class CosineAnnealingWithWarmup:
    """Cosine annealing learning rate scheduler with warmup"""
    def __init__(self, optimizer, warmup_epochs, total_epochs,
                 warmup_start_lr, base_lr, min_lr, steps_per_epoch):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.warmup_start_lr = warmup_start_lr
        self.base_lr = base_lr
        self.min_lr = min_lr
        self.steps_per_epoch = steps_per_epoch

        self.warmup_steps = warmup_epochs * steps_per_epoch
        self.total_steps = total_epochs * steps_per_epoch
        self.current_step = 0

    def step(self):
        """Update learning rate"""
        if self.current_step < self.warmup_steps:
            # Linear warmup
            lr = self.warmup_start_lr + (self.base_lr - self.warmup_start_lr) * \
                 (self.current_step / self.warmup_steps)
        else:
            # Cosine annealing
            progress = (self.current_step - self.warmup_steps) / \
                      (self.total_steps - self.warmup_steps)
            lr = self.min_lr + (self.base_lr - self.min_lr) * \
                 0.5 * (1 + math.cos(math.pi * progress))

        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

        self.current_step += 1
        return lr

    def get_lr(self):
        """Get current learning rate"""
        return self.optimizer.param_groups[0]['lr']

# ============================================================================
# SUNDEW ALGORITHM - FIXED SIGNIFICANCE SCORING
# ============================================================================

class SundewAlgorithm:
    """Adaptive gating with FIXED gradient-magnitude based significance"""

    def __init__(self, config):
        self.target_activation_rate = config.target_activation_rate
        self.activation_threshold = config.initial_threshold

        # PI controller
        self.kp = config.adapt_kp
        self.ki = config.adapt_ki
        self.integral_error = 0.0

        # EMA smoothing
        self.ema_alpha = config.ema_alpha
        self.activation_rate_ema = config.target_activation_rate

        # Energy tracking
        self.energy_per_activation = config.energy_per_activation
        self.energy_per_skip = config.energy_per_skip
        self.total_baseline_energy = 0.0
        self.total_actual_energy = 0.0

    def compute_significance(self, losses, outputs):
        """
        üîß FIXED SIGNIFICANCE SCORING

        OLD (BROKEN):
        - Used batch-normalized loss ‚Üí collapsed to ~1.0 ¬± 0.2
        - Used image std ‚Üí meaningless for ImageNet

        NEW (FIXED):
        - Use ABSOLUTE loss magnitude (harder samples = higher loss)
        - Use prediction entropy (uncertain samples = high entropy)
        - No batch normalization that destroys variance!
        """
        batch_size = losses.size(0)

        # Factor 1: Loss magnitude (scale to [0,1] using max, NOT mean!)
        # Higher loss = harder sample = more important
        loss_max = losses.max()
        if loss_max > 0:
            loss_significance = losses / loss_max
        else:
            loss_significance = torch.ones_like(losses)

        # Factor 2: Prediction uncertainty (entropy)
        # High entropy = model is uncertain = important sample
        probs = torch.softmax(outputs, dim=1)
        entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=1)
        entropy_max = entropy.max()
        if entropy_max > 0:
            entropy_significance = entropy / entropy_max
        else:
            entropy_significance = torch.zeros_like(entropy)

        # Weighted combination: 70% loss, 30% uncertainty
        significance = 0.7 * loss_significance + 0.3 * entropy_significance

        return significance

    def select_samples(self, losses, images, outputs):
        """Select important samples - UPDATED to use outputs for entropy"""
        batch_size = losses.size(0)

        # Compute significance scores (FIXED VERSION)
        significance = self.compute_significance(losses, outputs)

        # Select samples above threshold
        active_mask = significance > self.activation_threshold
        num_active = active_mask.sum().item()

        # Fallback mechanism
        if num_active == 0:
            random_indices = torch.randperm(batch_size)[:2]
            active_mask[random_indices] = True
            num_active = 2

        # Update activation rate EMA
        current_activation_rate = num_active / batch_size
        self.activation_rate_ema = (
            self.ema_alpha * current_activation_rate +
            (1 - self.ema_alpha) * self.activation_rate_ema
        )

        # PI controller (same as before)
        error = self.activation_rate_ema - self.target_activation_rate
        proportional = self.kp * error

        # Integral with anti-windup
        if 0.01 < self.activation_threshold < 10.0:
            self.integral_error += error
            self.integral_error = max(-50, min(50, self.integral_error))
        else:
            self.integral_error *= 0.90

        new_threshold = self.activation_threshold + proportional + self.ki * self.integral_error
        self.activation_threshold = max(0.01, new_threshold)

        # Energy tracking
        baseline_energy = batch_size * self.energy_per_activation
        actual_energy = (num_active * self.energy_per_activation +
                        (batch_size - num_active) * self.energy_per_skip)

        self.total_baseline_energy += baseline_energy
        self.total_actual_energy += actual_energy

        # Calculate energy savings
        energy_savings = 0.0
        if self.total_baseline_energy > 0:
            energy_savings = ((self.total_baseline_energy - self.total_actual_energy) /
                             self.total_baseline_energy * 100)

        energy_info = {
            'num_active': num_active,
            'activation_rate': current_activation_rate,
            'activation_rate_ema': self.activation_rate_ema,
            'threshold': self.activation_threshold,
            'energy_savings': energy_savings,
        }

        return active_mask, energy_info

# ============================================================================
# TRAINING
# ============================================================================

def train_epoch_ast(model, train_loader, criterion, optimizer, scheduler, sundew, config, epoch):
    """Train one epoch with AST and LR scheduling"""
    model.train()

    running_loss = 0.0
    total_samples = 0
    total_active = 0

    epoch_stats = {
        'activation_rates': [],
        'thresholds': [],
        'energy_savings': [],
        'learning_rates': [],  # üÜï Track LR
    }

    for batch_idx, (images, labels) in enumerate(train_loader):
        images = images.to(config.device)
        labels = labels.to(config.device)
        batch_size = images.size(0)

        # Forward pass to get losses AND outputs (for entropy calculation)
        with torch.no_grad():
            outputs = model(images)
            losses = torch.nn.functional.cross_entropy(outputs, labels, reduction='none')

        # Select important samples (FIXED VERSION uses outputs)
        active_mask, energy_info = sundew.select_samples(losses, images, outputs)

        # Train only on active samples
        if active_mask.sum() > 0:
            active_images = images[active_mask]
            active_labels = labels[active_mask]

            optimizer.zero_grad()
            active_outputs = model(active_images)
            loss = criterion(active_outputs, active_labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * active_mask.sum().item()
            total_active += active_mask.sum().item()

        # üÜï Update learning rate
        current_lr = scheduler.step()

        total_samples += batch_size

        # Track stats
        epoch_stats['activation_rates'].append(energy_info['activation_rate_ema'])
        epoch_stats['thresholds'].append(energy_info['threshold'])
        epoch_stats['energy_savings'].append(energy_info['energy_savings'])
        epoch_stats['learning_rates'].append(current_lr)

        # Print progress
        if (batch_idx + 1) % 50 == 0:
            print(f"  Batch {batch_idx+1:4d}/{len(train_loader)} | "
                  f"Act: {100*energy_info['activation_rate_ema']:4.1f}% | "
                  f"‚ö° Energy Saved: {energy_info['energy_savings']:5.1f}% | "
                  f"Threshold: {energy_info['threshold']:.3f} | "
                  f"LR: {current_lr:.6f}")

    avg_loss = running_loss / max(total_active, 1)
    avg_activation = total_active / total_samples

    return avg_loss, avg_activation, epoch_stats

def validate(model, val_loader, criterion, config):
    """Validation loop"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(config.device)
            labels = labels.to(config.device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    avg_loss = running_loss / total
    accuracy = 100.0 * correct / total

    return avg_loss, accuracy

# ============================================================================
# ARCHITECTURE DIAGRAM WITH LR SCHEDULE
# ============================================================================

def create_architecture_diagram():
    """Create AST architecture diagram with LR scheduling highlighted"""
    fig, ax = plt.subplots(figsize=(16, 10))
    ax.axis('off')

    # Title
    fig.suptitle('üöÄ MEDIUM TEST - LR Schedule + RandAugment (ImageNet-100)',
                 fontsize=20, fontweight='bold', y=0.98)

    # Define positions
    y_start = 0.85
    box_height = 0.08
    box_width = 0.18

    # Row 1: Input Stage
    ax.add_patch(plt.Rectangle((0.05, y_start), box_width, box_height,
                                facecolor='lightblue', edgecolor='black', linewidth=2))
    ax.text(0.14, y_start + box_height/2, 'Input Batch\n[64, 3, 224, 224]',
            ha='center', va='center', fontsize=12, fontweight='bold')

    # Arrow
    ax.arrow(0.23, y_start + box_height/2, 0.05, 0, head_width=0.02,
             head_length=0.02, fc='black', ec='black', linewidth=2)

    # FIXED Significance (highlighted)
    ax.add_patch(plt.Rectangle((0.30, y_start), box_width, box_height,
                                facecolor='lime', edgecolor='red', linewidth=3))
    ax.text(0.39, y_start + box_height/2, '‚úÖ FIXED\nSignificance\n(Loss + Entropy)',
            ha='center', va='center', fontsize=11, fontweight='bold', color='darkgreen')

    # Arrow
    ax.arrow(0.48, y_start + box_height/2, 0.05, 0, head_width=0.02,
             head_length=0.02, fc='black', ec='black', linewidth=2)

    # Sundew Gating
    ax.add_patch(plt.Rectangle((0.55, y_start), box_width, box_height,
                                facecolor='yellow', edgecolor='black', linewidth=2))
    ax.text(0.64, y_start + box_height/2, 'Sundew Gating\n‚ö° Energy Tracking',
            ha='center', va='center', fontsize=11, fontweight='bold')

    # Arrow
    ax.arrow(0.73, y_start + box_height/2, 0.05, 0, head_width=0.02,
             head_length=0.02, fc='black', ec='black', linewidth=2)

    # Active Mask
    ax.add_patch(plt.Rectangle((0.80, y_start), box_width, box_height,
                                facecolor='orange', edgecolor='black', linewidth=2))
    ax.text(0.89, y_start + box_height/2, 'Active Mask\n[64] ‚Üí [~6]',
            ha='center', va='center', fontsize=11, fontweight='bold')

    # Row 2: Batched Training with LR Schedule
    y_row2 = y_start - 0.15

    ax.add_patch(plt.Rectangle((0.30, y_row2), 0.43, box_height,
                                facecolor='lightcoral', edgecolor='black', linewidth=3))
    ax.text(0.515, y_row2 + box_height/2,
            'Batched ResNet50 Training + üÜï Cosine LR Schedule\n(GPU Parallel on ~6 Active Samples)',
            ha='center', va='center', fontsize=12, fontweight='bold')

    # Arrow down
    ax.arrow(0.89, y_start, 0, -0.06, head_width=0.02,
             head_length=0.02, fc='black', ec='black', linewidth=2)

    # Row 3: What Changed
    y_row3 = y_row2 - 0.20

    # Box 1: Significance Fix
    ax.add_patch(plt.Rectangle((0.05, y_row3), 0.27, 0.12,
                                facecolor='#E6FFE6', edgecolor='green', linewidth=2))
    ax.text(0.185, y_row3 + 0.08, '‚úÖ FIXED: Significance Scoring',
            ha='center', va='center', fontsize=11, fontweight='bold', color='green')
    ax.text(0.185, y_row3 + 0.04, 'sig = 0.7√ó(loss/max) + 0.3√óentropy\n‚Üí preserves variance',
            ha='center', va='center', fontsize=9)

    # Box 2: LR Schedule
    ax.add_patch(plt.Rectangle((0.35, y_row3), 0.27, 0.12,
                                facecolor='#E6F3FF', edgecolor='blue', linewidth=2))
    ax.text(0.485, y_row3 + 0.08, 'üÜï NEW: Cosine Annealing LR',
            ha='center', va='center', fontsize=11, fontweight='bold', color='blue')
    ax.text(0.485, y_row3 + 0.04, '1 epoch warmup + cosine decay\n‚Üí better convergence',
            ha='center', va='center', fontsize=9)

    # Box 3: RandAugment
    ax.add_patch(plt.Rectangle((0.65, y_row3), 0.27, 0.12,
                                facecolor='#FFF0E6', edgecolor='orange', linewidth=2))
    ax.text(0.785, y_row3 + 0.08, 'üÜï NEW: RandAugment',
            ha='center', va='center', fontsize=11, fontweight='bold', color='darkorange')
    ax.text(0.785, y_row3 + 0.04, 'AutoAugment policy\n‚Üí stronger regularization',
            ha='center', va='center', fontsize=9)

    # Bottom: Test Info
    y_bottom = 0.08

    ax.add_patch(plt.Rectangle((0.25, y_bottom), 0.5, 0.10,
                                facecolor='lightblue', edgecolor='blue', linewidth=3))
    ax.text(0.5, y_bottom + 0.05, 'üöÄ MEDIUM TEST: 5 epochs | ~30 min | LR Schedule + RandAugment',
            ha='center', va='center', fontsize=13, fontweight='bold')

    plt.xlim(0, 1)
    plt.ylim(0, 1)

    plt.savefig('mediumtest_architecture.png', dpi=150, bbox_inches='tight', facecolor='white')
    print(f"üèóÔ∏è Architecture diagram saved to: mediumtest_architecture.png")
    plt.show()

# ============================================================================
# RESULTS VISUALIZATION WITH LR PROGRESSION
# ============================================================================

def create_results_dashboard(all_epoch_stats, final_results, config):
    """Create comprehensive results dashboard with LR progression"""

    fig = plt.figure(figsize=(18, 14))
    gs = GridSpec(4, 3, figure=fig, hspace=0.3, wspace=0.3)

    # Combine all epoch stats
    all_activation_rates = []
    all_thresholds = []
    all_energy_savings = []
    all_learning_rates = []

    for epoch_stats in all_epoch_stats:
        all_activation_rates.extend(epoch_stats['activation_rates'])
        all_thresholds.extend(epoch_stats['thresholds'])
        all_energy_savings.extend(epoch_stats['energy_savings'])
        all_learning_rates.extend(epoch_stats['learning_rates'])

    batches = list(range(len(all_activation_rates)))

    # Plot 1: Activation Rate
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.plot(batches, [r * 100 for r in all_activation_rates],
             color='blue', linewidth=1.5, alpha=0.7, label='Activation Rate')
    ax1.axhline(y=config.target_activation_rate * 100, color='red',
                linestyle='--', linewidth=2, label=f'Target: {config.target_activation_rate*100:.0f}%')
    ax1.set_xlabel('Batch', fontsize=11, fontweight='bold')
    ax1.set_ylabel('Activation Rate (%)', fontsize=11, fontweight='bold')
    ax1.set_title('üéØ Activation Rate Convergence', fontsize=13, fontweight='bold')
    ax1.legend(fontsize=9)
    ax1.grid(True, alpha=0.3)

    # Plot 2: Threshold
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.plot(batches, all_thresholds, color='green', linewidth=1.5, alpha=0.7)
    ax2.set_xlabel('Batch', fontsize=11, fontweight='bold')
    ax2.set_ylabel('Threshold', fontsize=11, fontweight='bold')
    ax2.set_title('üéõÔ∏è PI Controller Threshold', fontsize=13, fontweight='bold')
    ax2.grid(True, alpha=0.3)

    # Plot 3: Energy Savings
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.plot(batches, all_energy_savings, color='orange', linewidth=1.5, alpha=0.7, label='Energy Saved')
    ax3.axhline(y=90, color='red', linestyle='--', linewidth=2, label='Target: 90%')
    ax3.set_xlabel('Batch', fontsize=11, fontweight='bold')
    ax3.set_ylabel('Energy Savings (%)', fontsize=11, fontweight='bold')
    ax3.set_title('‚ö° Cumulative Energy Savings', fontsize=13, fontweight='bold')
    ax3.legend(fontsize=9)
    ax3.grid(True, alpha=0.3)

    # üÜï Plot 4: Learning Rate Schedule
    ax4 = fig.add_subplot(gs[1, 0])
    ax4.plot(batches, all_learning_rates, color='purple', linewidth=2, alpha=0.8)
    ax4.set_xlabel('Batch', fontsize=11, fontweight='bold')
    ax4.set_ylabel('Learning Rate', fontsize=11, fontweight='bold')
    ax4.set_title('üìà LR Schedule (Warmup + Cosine)', fontsize=13, fontweight='bold')
    ax4.set_yscale('log')
    ax4.grid(True, alpha=0.3)

    # Plot 5: Activation Distribution
    ax5 = fig.add_subplot(gs[1, 1])
    ax5.hist([r * 100 for r in all_activation_rates], bins=50,
             color='skyblue', edgecolor='black', alpha=0.7)
    ax5.axvline(x=config.target_activation_rate * 100, color='red',
                linestyle='--', linewidth=2, label='Target')
    ax5.set_xlabel('Activation Rate (%)', fontsize=11, fontweight='bold')
    ax5.set_ylabel('Frequency', fontsize=11, fontweight='bold')
    ax5.set_title('üìä Activation Rate Distribution', fontsize=13, fontweight='bold')
    ax5.legend(fontsize=9)
    ax5.grid(True, alpha=0.3, axis='y')

    # Plot 6: Energy Distribution
    ax6 = fig.add_subplot(gs[1, 2])
    ax6.hist(all_energy_savings, bins=50, color='lightcoral',
             edgecolor='black', alpha=0.7)
    ax6.axvline(x=90, color='red', linestyle='--', linewidth=2, label='Target')
    ax6.set_xlabel('Energy Savings (%)', fontsize=11, fontweight='bold')
    ax6.set_ylabel('Frequency', fontsize=11, fontweight='bold')
    ax6.set_title('‚ö° Energy Savings Distribution', fontsize=13, fontweight='bold')
    ax6.legend(fontsize=9)
    ax6.grid(True, alpha=0.3, axis='y')

    # Plot 7: Training Progress
    ax7 = fig.add_subplot(gs[2, 0])
    epochs_plot = list(range(1, len(all_epoch_stats) + 1))
    epoch_energy_savings = []
    if len(all_epoch_stats) > 0:
        batch_size = len(all_epoch_stats[0]['energy_savings'])
        for i in range(len(all_epoch_stats)):
            start_idx = i * batch_size
            end_idx = min(start_idx + batch_size, len(all_energy_savings))
            avg = np.mean(all_energy_savings[start_idx:end_idx])
            epoch_energy_savings.append(avg)

    ax7.plot(epochs_plot, epoch_energy_savings, 'o-', color='purple',
             linewidth=2, markersize=8, label='Energy Saved')
    ax7.set_xlabel('Epoch', fontsize=11, fontweight='bold')
    ax7.set_ylabel('Energy Savings (%)', fontsize=11, fontweight='bold')
    ax7.set_title('üìà Training Progress', fontsize=13, fontweight='bold')
    ax7.legend(fontsize=9)
    ax7.grid(True, alpha=0.3)

    # üÜï Plot 8: LR vs Epoch
    ax8 = fig.add_subplot(gs[2, 1])
    epoch_avg_lrs = []
    if len(all_epoch_stats) > 0:
        batch_size = len(all_epoch_stats[0]['learning_rates'])
        for i in range(len(all_epoch_stats)):
            start_idx = i * batch_size
            end_idx = min(start_idx + batch_size, len(all_learning_rates))
            avg = np.mean(all_learning_rates[start_idx:end_idx])
            epoch_avg_lrs.append(avg)

    ax8.plot(epochs_plot, epoch_avg_lrs, 'o-', color='green',
             linewidth=2, markersize=8, label='Avg LR')
    ax8.set_xlabel('Epoch', fontsize=11, fontweight='bold')
    ax8.set_ylabel('Learning Rate', fontsize=11, fontweight='bold')
    ax8.set_title('üìä LR per Epoch', fontsize=13, fontweight='bold')
    ax8.set_yscale('log')
    ax8.legend(fontsize=9)
    ax8.grid(True, alpha=0.3)

    # Plot 9: LR Distribution
    ax9 = fig.add_subplot(gs[2, 2])
    ax9.hist(all_learning_rates, bins=50, color='lightgreen',
             edgecolor='black', alpha=0.7)
    ax9.set_xlabel('Learning Rate', fontsize=11, fontweight='bold')
    ax9.set_ylabel('Frequency', fontsize=11, fontweight='bold')
    ax9.set_title('üìä LR Distribution', fontsize=13, fontweight='bold')
    ax9.set_xscale('log')
    ax9.grid(True, alpha=0.3, axis='y')

    # Summary Box
    ax10 = fig.add_subplot(gs[3, :])
    ax10.axis('off')

    summary_text = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                       üöÄ MEDIUM TEST RESULTS (5 EPOCHS) üöÄ                                 ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                                            ‚ïë
‚ïë  üìä Validation Accuracy:      {final_results['accuracy']:6.2f}%     {'‚úÖ GREAT!' if final_results['accuracy'] >= 35 else '‚ö†Ô∏è  Check logs'}                           ‚ïë
‚ïë  ‚ö° Energy Savings:           {final_results['energy_savings']:6.2f}%     {'‚úÖ ON TARGET' if final_results['energy_savings'] >= 88 else '‚ö†Ô∏è  LOW'}                              ‚ïë
‚ïë  üéØ Activation Rate:          {final_results['activation_rate']:6.2f}%     {'‚úÖ PERFECT' if 9 <= final_results['activation_rate'] <= 12 else '‚ö†Ô∏è  OFF TARGET'}                              ‚ïë
‚ïë  ‚è±Ô∏è  Training Time:            {final_results['training_time']:6.1f} min                                              ‚ïë
‚ïë  üìà Final LR:                 {final_results['final_lr']:.6f}                                              ‚ïë
‚ïë                                                                                            ‚ïë
‚ïë  üî¨ NEW FEATURES:                                                                          ‚ïë
‚ïë     üÜï Cosine annealing LR with {config.lr_warmup_epochs} epoch warmup                                     ‚ïë
‚ïë     üÜï RandAugment (AutoAugment policy) for stronger regularization                       ‚ïë
‚ïë     ‚úÖ Fixed significance scoring (loss magnitude + entropy)                              ‚ïë
‚ïë                                                                                            ‚ïë
‚ïë  üéØ SUCCESS CRITERIA:                                                                      ‚ïë
‚ïë     ‚úÖ Accuracy > 35%  ‚Üí LR schedule helps! Proceed to Production                         ‚ïë
‚ïë     ‚ö†Ô∏è  Accuracy < 33%  ‚Üí May need more tuning                                             ‚ïë
‚ïë                                                                                            ‚ïë
‚ïë  üì¶ Dataset:  ImageNet-100 | ü§ñ Model: ResNet50 ({final_results['num_params']:.1f}M) | üî¨ Epochs: {config.num_epochs}            ‚ïë
‚ïë                                                                                            ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """

    ax10.text(0.5, 0.5, summary_text, fontsize=11, family='monospace',
             ha='center', va='center',
             bbox=dict(boxstyle='round,pad=1', facecolor='lightyellow',
                      edgecolor='orange', linewidth=3))

    plt.suptitle('üöÄ Medium Test - LR Schedule + RandAugment Validation',
                 fontsize=16, fontweight='bold', y=0.98)

    plt.savefig('mediumtest_results.png', dpi=150, bbox_inches='tight', facecolor='white')
    print("\n‚úÖ Results dashboard saved: mediumtest_results.png")
    plt.show()

# ============================================================================
# MAIN
# ============================================================================

def main():
    """Main training function"""

    print("=" * 70)
    print("üöÄ MEDIUM TEST - LR SCHEDULE + RANDAUGMENT")
    print("Testing for 5 epochs (~30 minutes)")
    print("=" * 70)

    config = Config()
    print(f"Device: {config.device}")
    print(f"Target activation rate: {config.target_activation_rate*100:.1f}%")
    print(f"LR: {config.learning_rate} ‚Üí {config.lr_min} (cosine annealing)")
    print(f"Warmup epochs: {config.lr_warmup_epochs}")
    print(f"Data augmentation: RandAugment (AutoAugment policy)")
    print()

    # Create architecture diagram
    print("üìê Generating architecture diagram...")
    create_architecture_diagram()
    print()

    # Load data
    train_loader, val_loader = get_dataloaders(config)
    print()

    # Load model
    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
    model.fc = nn.Linear(model.fc.in_features, config.num_classes)
    model = model.to(config.device)
    num_params = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"Loaded ResNet50 ({num_params:.1f}M params)")
    print()

    # Setup
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate,
                           weight_decay=config.weight_decay)

    # üÜï Create LR scheduler
    scheduler = CosineAnnealingWithWarmup(
        optimizer=optimizer,
        warmup_epochs=config.lr_warmup_epochs,
        total_epochs=config.num_epochs,
        warmup_start_lr=config.lr_min,
        base_lr=config.learning_rate,
        min_lr=config.lr_min,
        steps_per_epoch=len(train_loader)
    )

    sundew = SundewAlgorithm(config)

    # Training loop
    start_time = time.time()
    best_accuracy = 0.0
    all_epoch_stats = []

    for epoch in range(1, config.num_epochs + 1):
        epoch_start = time.time()

        print()
        print("=" * 60)
        print(f"Epoch {epoch}/{config.num_epochs} | LR: {scheduler.get_lr():.6f}")
        print("=" * 60)

        # Train
        train_loss, train_activation, epoch_stats = train_epoch_ast(
            model, train_loader, criterion, optimizer, scheduler, sundew, config, epoch
        )
        all_epoch_stats.append(epoch_stats)

        # Validate
        val_loss, val_accuracy = validate(model, val_loader, criterion, config)

        # Energy savings
        energy_savings = 0.0
        if sundew.total_baseline_energy > 0:
            energy_savings = ((sundew.total_baseline_energy - sundew.total_actual_energy) /
                             sundew.total_baseline_energy * 100)

        epoch_time = time.time() - epoch_start

        # Print summary
        print()
        print(f"‚úÖ Epoch {epoch} Complete | "
              f"Val Acc: {val_accuracy:5.2f}% | "
              f"Loss: {train_loss:.4f} | "
              f"‚ö° Energy Saved: {energy_savings:5.1f}% | "
              f"LR: {scheduler.get_lr():.6f} | "
              f"Time: {epoch_time:.1f}s")

        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy

    total_time = time.time() - start_time

    # Final results
    print()
    print("=" * 70)
    print("üöÄ MEDIUM TEST RESULTS")
    print("=" * 70)
    print(f"Best Validation Accuracy: {best_accuracy:.2f}%")
    print(f"Total Energy Savings: {energy_savings:.2f}%")
    print(f"Average Activation Rate: {100*train_activation:.2f}%")
    print(f"Final Learning Rate: {scheduler.get_lr():.6f}")
    print(f"Total Training Time: {total_time/60:.1f} minutes")
    print()

    # Comparison
    print("üìä COMPARISON:")
    print(f"   Quick Test (no LR schedule): ~32-38%")
    print(f"   Medium Test (with LR):        {best_accuracy:.2f}%")
    print(f"   Improvement:                  {best_accuracy - 35:+.2f}% (vs expected 35%)")
    print()

    if best_accuracy >= 35:
        print("‚úÖ SUCCESS! LR schedule + RandAugment work!")
        print("   ‚Üí Proceed to PRODUCTION version (80 epochs)")
    elif best_accuracy >= 32:
        print("‚ö†Ô∏è  PARTIAL SUCCESS - slight improvement over Quick Test")
        print("   ‚Üí Consider testing with more epochs")
    else:
        print("‚ùå NO IMPROVEMENT - LR schedule may need tuning")
    print("=" * 70)

    # Create dashboard
    final_results = {
        'accuracy': best_accuracy,
        'energy_savings': energy_savings,
        'activation_rate': 100 * train_activation,
        'training_time': total_time / 60,
        'final_lr': scheduler.get_lr(),
        'num_params': num_params,
    }

    print("\nüìä Generating results dashboard...")
    create_results_dashboard(all_epoch_stats, final_results, config)

if __name__ == "__main__":
    main()
