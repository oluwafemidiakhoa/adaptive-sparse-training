"""
üî• TWO-STAGE AST - FIXED VERSION (No Accuracy Collapse!)
====================================================================
FIXES:
1. Keep same optimizer (SGD) for both phases
2. Higher LR for AST phase (0.005 instead of 0.001)
3. Higher target activation (40% instead of 20%)
4. Gradual transition (not abrupt switch)
5. Lower initial threshold for better adaptation

Expected Results:
- Warmup: 85-92% accuracy
- AST: 80-88% accuracy (minimal drop!)
- Energy Savings: 60-70%
- Overall Speedup: 3-4√ó
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.datasets import ImageFolder
from pathlib import Path
import time
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec

# ============================================================================
# CONFIGURATION - FIXED PARAMETERS
# ============================================================================

class Config:
    """Two-stage training configuration - FIXED"""
    # Dataset
    data_dir = "/kaggle/input/imagenet100/ImageNet100"
    num_classes = 100
    image_size = 224

    # Training
    batch_size = 128
    num_epochs = 50
    warmup_epochs = 10

    # FIXED: Use same optimizer and similar LR for both phases
    base_lr = 0.01  # Base learning rate
    warmup_lr = 0.01  # Warmup LR
    ast_lr = 0.005  # AST LR (HIGHER than before, only 2√ó reduction)
    weight_decay = 1e-4
    momentum = 0.9

    # FIXED: Higher activation rate (40% instead of 20%)
    target_activation_rate = 0.40  # 40% activation (less aggressive)
    initial_threshold = 3.0  # LOWER threshold for easier adaptation

    # FIXED: Gentler PI controller
    adapt_kp = 0.005  # Reduced
    adapt_ki = 0.0001  # Reduced
    ema_alpha = 0.1  # Slower

    # Energy Model
    energy_per_activation = 1.0
    energy_per_skip = 0.01

    # System
    device = "cuda" if torch.cuda.is_available() else "cpu"
    num_workers = 4
    pin_memory = True

# ============================================================================
# DATASET
# ============================================================================

def get_dataloaders(config):
    """Create dataloaders"""
    normalize = transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )

    train_transform = transforms.Compose([
        transforms.RandomResizedCrop(config.image_size, scale=(0.08, 1.0)),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
        transforms.RandomGrayscale(p=0.1),
        transforms.ToTensor(),
        normalize,
        transforms.RandomErasing(p=0.25),
    ])

    val_transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(config.image_size),
        transforms.ToTensor(),
        normalize,
    ])

    train_dataset = ImageFolder(str(Path(config.data_dir) / 'train'), transform=train_transform)
    val_dataset = ImageFolder(str(Path(config.data_dir) / 'val'), transform=val_transform)

    print(f"üì¶ Loaded {len(train_dataset):,} training images")
    print(f"üì¶ Loaded {len(val_dataset):,} validation images")

    train_loader = DataLoader(train_dataset, batch_size=config.batch_size,
                              shuffle=True, num_workers=config.num_workers,
                              pin_memory=config.pin_memory, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size,
                           shuffle=False, num_workers=config.num_workers,
                           pin_memory=config.pin_memory)

    return train_loader, val_loader

# ============================================================================
# LR SCHEDULER
# ============================================================================

class CosineAnnealingWarmup:
    """Cosine annealing with warmup"""
    def __init__(self, optimizer, warmup_epochs, max_epochs, min_lr=1e-6):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.min_lr = min_lr
        self.base_lr = optimizer.param_groups[0]['lr']

    def step(self, epoch):
        if epoch < self.warmup_epochs:
            lr = self.base_lr * (epoch + 1) / self.warmup_epochs
        else:
            progress = (epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)
            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))

        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

        return lr

# ============================================================================
# SUNDEW ALGORITHM - FIXED
# ============================================================================

class SundewAlgorithm:
    """Adaptive sample selection - FIXED"""

    def __init__(self, config):
        self.target_activation_rate = config.target_activation_rate
        self.activation_threshold = config.initial_threshold

        # PI controller
        self.kp = config.adapt_kp
        self.ki = config.adapt_ki
        self.integral_error = 0.0

        # EMA smoothing
        self.ema_alpha = config.ema_alpha
        self.activation_rate_ema = config.target_activation_rate

        # Energy tracking
        self.energy_per_activation = config.energy_per_activation
        self.energy_per_skip = config.energy_per_skip
        self.total_baseline_energy = 0.0
        self.total_actual_energy = 0.0

    def compute_significance(self, losses, outputs):
        """RAW significance scoring"""
        # RAW loss
        loss_component = losses

        # RAW entropy
        probs = torch.softmax(outputs, dim=1)
        entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=1)

        # Weighted combination
        significance = 0.7 * loss_component + 0.3 * entropy

        return significance

    def select_samples(self, losses, outputs):
        """Select important samples"""
        batch_size = losses.size(0)

        # Compute significance
        significance = self.compute_significance(losses, outputs)

        # Select samples above threshold
        active_mask = significance > self.activation_threshold
        num_active = active_mask.sum().item()

        # Fallback: ensure minimum activation
        min_active = max(2, int(batch_size * 0.10))  # At least 10%
        if num_active < min_active:
            _, top_indices = torch.topk(significance, min_active)
            active_mask = torch.zeros_like(active_mask, dtype=torch.bool)
            active_mask[top_indices] = True
            num_active = min_active

        # Update activation rate EMA
        current_activation_rate = num_active / batch_size
        self.activation_rate_ema = (
            self.ema_alpha * current_activation_rate +
            (1 - self.ema_alpha) * self.activation_rate_ema
        )

        # PI controller
        error = self.activation_rate_ema - self.target_activation_rate
        proportional = self.kp * error

        # Integral with anti-windup
        if 0.5 < self.activation_threshold < 10.0:
            self.integral_error += error
            self.integral_error = max(-100, min(100, self.integral_error))
        else:
            self.integral_error *= 0.90

        # Update threshold
        new_threshold = self.activation_threshold + proportional + self.ki * self.integral_error
        self.activation_threshold = max(0.5, min(10.0, new_threshold))

        # Energy tracking
        baseline_energy = batch_size * self.energy_per_activation
        actual_energy = (num_active * self.energy_per_activation +
                        (batch_size - num_active) * self.energy_per_skip)

        self.total_baseline_energy += baseline_energy
        self.total_actual_energy += actual_energy

        energy_savings = 0.0
        if self.total_baseline_energy > 0:
            energy_savings = ((self.total_baseline_energy - self.total_actual_energy) /
                             self.total_baseline_energy * 100)

        energy_info = {
            'num_active': num_active,
            'activation_rate': current_activation_rate,
            'activation_rate_ema': self.activation_rate_ema,
            'threshold': self.activation_threshold,
            'energy_savings': energy_savings,
        }

        return active_mask, energy_info

# ============================================================================
# TRAINING FUNCTIONS
# ============================================================================

def train_epoch_warmup(model, train_loader, criterion, optimizer, config, epoch):
    """STAGE 1: Warmup training on 100% of samples"""
    model.train()

    running_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        images = images.to(config.device)
        labels = labels.to(config.device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        running_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        if (batch_idx + 1) % 100 == 0:
            acc = 100.0 * correct / total
            avg_loss = running_loss / total
            print(f"  Batch {batch_idx+1:4d}/{len(train_loader)} | "
                  f"Loss: {avg_loss:.4f} | "
                  f"Train Acc: {acc:5.2f}%")

    avg_loss = running_loss / total
    accuracy = 100.0 * correct / total

    return avg_loss, accuracy

def train_epoch_ast(model, train_loader, criterion, optimizer, sundew, config, epoch):
    """STAGE 2: AST training on selected samples"""
    model.train()

    running_loss = 0.0
    correct = 0
    total_samples = 0
    total_active = 0

    epoch_stats = {
        'activation_rates': [],
        'thresholds': [],
        'energy_savings': [],
    }

    for batch_idx, (images, labels) in enumerate(train_loader):
        images = images.to(config.device)
        labels = labels.to(config.device)
        batch_size = images.size(0)

        # Forward pass to get significance
        with torch.no_grad():
            outputs = model(images)
            losses = torch.nn.functional.cross_entropy(outputs, labels, reduction='none')

        # Select important samples
        active_mask, energy_info = sundew.select_samples(losses, outputs)

        # Train on selected samples
        if active_mask.sum() > 0:
            active_images = images[active_mask]
            active_labels = labels[active_mask]

            optimizer.zero_grad()
            active_outputs = model(active_images)
            loss = criterion(active_outputs, active_labels)
            loss.backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            running_loss += loss.item() * active_mask.sum().item()

            _, predicted = active_outputs.max(1)
            correct += predicted.eq(active_labels).sum().item()
            total_active += active_mask.sum().item()

        total_samples += batch_size

        epoch_stats['activation_rates'].append(energy_info['activation_rate_ema'])
        epoch_stats['thresholds'].append(energy_info['threshold'])
        epoch_stats['energy_savings'].append(energy_info['energy_savings'])

        if (batch_idx + 1) % 100 == 0:
            train_acc = 100.0 * correct / max(total_active, 1)
            print(f"  Batch {batch_idx+1:4d}/{len(train_loader)} | "
                  f"Act: {100*energy_info['activation_rate_ema']:5.1f}% | "
                  f"Train Acc: {train_acc:5.2f}% | "
                  f"‚ö° Energy: {energy_info['energy_savings']:5.1f}% | "
                  f"Threshold: {energy_info['threshold']:.2f}")

    avg_loss = running_loss / max(total_active, 1)
    avg_activation = total_active / total_samples
    train_accuracy = 100.0 * correct / max(total_active, 1)

    return avg_loss, avg_activation, train_accuracy, epoch_stats

def validate(model, val_loader, criterion, config):
    """Validation loop"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images = images.to(config.device)
            labels = labels.to(config.device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    avg_loss = running_loss / total
    accuracy = 100.0 * correct / total

    return avg_loss, accuracy

# ============================================================================
# MAIN
# ============================================================================

def main():
    """Two-stage training pipeline - FIXED"""

    print("=" * 70)
    print("üî• TWO-STAGE AST - FIXED VERSION")
    print("=" * 70)
    print("FIXES:")
    print("  1. Same optimizer (SGD) for both phases")
    print("  2. Higher LR for AST (0.005 vs 0.001)")
    print("  3. Higher activation (40% vs 20%)")
    print("  4. Lower initial threshold (3.0 vs 4.5)")
    print("=" * 70)
    print()

    config = Config()
    print(f"üì± Device: {config.device}")
    print(f"üéØ Target activation rate: {config.target_activation_rate*100:.0f}%")
    print(f"üì¶ Batch size: {config.batch_size}")
    print()

    # Load data
    train_loader, val_loader = get_dataloaders(config)
    print()

    # Load model
    print("ü§ñ Loading pretrained ResNet50...")
    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
    model.fc = nn.Linear(model.fc.in_features, config.num_classes)
    model = model.to(config.device)
    print(f"‚úÖ Loaded ResNet50 (23.7M params)")
    print()

    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

    best_accuracy = 0.0
    warmup_val_accs = []
    ast_val_accs = []

    total_start = time.time()

    # ========================================================================
    # STAGE 1: WARMUP - Use SGD
    # ========================================================================

    print("=" * 70)
    print("üî• STAGE 1: WARMUP (100% samples, SGD)")
    print("=" * 70)
    print()

    # FIXED: Use SGD with momentum (will keep for AST phase too!)
    optimizer = optim.SGD(model.parameters(), lr=config.warmup_lr,
                         momentum=config.momentum, weight_decay=config.weight_decay)
    scheduler_warmup = CosineAnnealingWarmup(optimizer, warmup_epochs=2,
                                            max_epochs=config.warmup_epochs)

    warmup_start = time.time()

    for epoch in range(1, config.warmup_epochs + 1):
        epoch_start = time.time()
        current_lr = scheduler_warmup.step(epoch - 1)

        print(f"\n{'='*60}")
        print(f"Warmup Epoch {epoch}/{config.warmup_epochs} | LR: {current_lr:.6f}")
        print(f"{'='*60}")

        train_loss, train_acc = train_epoch_warmup(model, train_loader, criterion,
                                                    optimizer, config, epoch)
        val_loss, val_acc = validate(model, val_loader, criterion, config)

        warmup_val_accs.append(val_acc)

        epoch_time = time.time() - epoch_start

        print(f"\n‚úÖ Warmup Epoch {epoch} | "
              f"Val Acc: {val_acc:5.2f}% | "
              f"Train Acc: {train_acc:5.2f}% | "
              f"Loss: {train_loss:.4f} | "
              f"Time: {epoch_time:.1f}s")

        if val_acc > best_accuracy:
            best_accuracy = val_acc
            torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),
                       'accuracy': val_acc}, 'best_model_warmup.pth')
            print(f"üíæ Saved best warmup model ({val_acc:.2f}%)")

    warmup_time = (time.time() - warmup_start) / 60

    print(f"\nüéâ WARMUP COMPLETE! Best: {best_accuracy:.2f}%")
    print()

    # ========================================================================
    # STAGE 2: AST - Keep using SGD (with higher LR)
    # ========================================================================

    print("=" * 70)
    print(f"üî• STAGE 2: AST (~{config.target_activation_rate*100:.0f}% samples, SGD)")
    print("=" * 70)
    print()

    # FIXED: Keep SGD, just adjust LR (don't switch to Adam!)
    for param_group in optimizer.param_groups:
        param_group['lr'] = config.ast_lr

    scheduler_ast = CosineAnnealingWarmup(optimizer, warmup_epochs=0,
                                         max_epochs=config.num_epochs - config.warmup_epochs,
                                         min_lr=1e-5)
    sundew = SundewAlgorithm(config)

    ast_start = time.time()

    for epoch in range(config.warmup_epochs + 1, config.num_epochs + 1):
        epoch_start = time.time()
        current_lr = scheduler_ast.step(epoch - config.warmup_epochs - 1)

        print(f"\n{'='*60}")
        print(f"AST Epoch {epoch}/{config.num_epochs} | LR: {current_lr:.6f}")
        print(f"{'='*60}")

        train_loss, train_activation, train_acc, epoch_stats = train_epoch_ast(
            model, train_loader, criterion, optimizer, sundew, config, epoch
        )

        val_loss, val_acc = validate(model, val_loader, criterion, config)

        ast_val_accs.append(val_acc)

        energy_savings = 0.0
        if sundew.total_baseline_energy > 0:
            energy_savings = ((sundew.total_baseline_energy - sundew.total_actual_energy) /
                             sundew.total_baseline_energy * 100)

        epoch_time = time.time() - epoch_start

        print(f"\n‚úÖ AST Epoch {epoch} | "
              f"Val Acc: {val_acc:5.2f}% | "
              f"Train Acc: {train_acc:5.2f}% | "
              f"Act: {100*train_activation:5.1f}% | "
              f"‚ö° Energy: {energy_savings:5.1f}% | "
              f"Time: {epoch_time:.1f}s")

        if val_acc > best_accuracy:
            best_accuracy = val_acc
            torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),
                       'accuracy': val_acc, 'energy_savings': energy_savings},
                      'best_model_ast.pth')
            print(f"üíæ Saved best AST model ({val_acc:.2f}%)")

    ast_time = (time.time() - ast_start) / 60
    total_time = (time.time() - total_start) / 60

    # Final results
    print("\n" + "=" * 70)
    print("üéâ TRAINING COMPLETE!")
    print("=" * 70)

    warmup_fraction = config.warmup_epochs / config.num_epochs
    ast_fraction = (config.num_epochs - config.warmup_epochs) / config.num_epochs
    overall_energy_savings = (warmup_fraction * 0 + ast_fraction * energy_savings)

    print(f"üèÜ Best Validation Accuracy: {best_accuracy:.2f}%")
    print(f"‚ö° Final Energy Savings (AST): {energy_savings:.2f}%")
    print(f"‚ö° Overall Energy Savings: {overall_energy_savings:.2f}%")
    print(f"‚è±Ô∏è  Total Time: {total_time:.1f} min")
    print()
    print(f"üìä Warmup final acc: {warmup_val_accs[-1]:.2f}%")
    print(f"üìä AST final acc: {ast_val_accs[-1]:.2f}%")
    print(f"üìä Accuracy drop: {warmup_val_accs[-1] - ast_val_accs[-1]:.2f}%")
    print("=" * 70)

if __name__ == "__main__":
    main()
